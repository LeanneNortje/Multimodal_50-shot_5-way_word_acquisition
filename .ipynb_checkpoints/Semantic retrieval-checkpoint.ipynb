{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "568e1f17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leannenortje/anaconda3/lib/python3.8/site-packages/torchvision/io/image.py:11: UserWarning: Failed to load image Python extension: /home/leannenortje/anaconda3/lib/python3.8/site-packages/torchvision/image.so: undefined symbol: _ZNK2at10TensorBase21__dispatch_contiguousEN3c1012MemoryFormatE\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from dataloaders import *\n",
    "from models.setup import *\n",
    "from torchsummary import summary\n",
    "from models.setup import *\n",
    "from models.ImageModels import *\n",
    "from models.AudioModels import *\n",
    "from dataloaders import *\n",
    "from itertools import chain\n",
    "import apex\n",
    "from apex import amp\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as trainable_parameters\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "import scipy\n",
    "import scipy.signal\n",
    "from scipy.spatial import distance\n",
    "import librosa\n",
    "import matplotlib.lines as lines\n",
    "\n",
    "import itertools\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3af24554",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tablePrinting(headings, row_headings, values):\n",
    "\n",
    "    assert(len(headings) - 1 == values.shape[-1])\n",
    "    assert(len(row_headings) == values.shape[0])\n",
    "\n",
    "    column_width = 10\n",
    "\n",
    "    heading = f''\n",
    "    for i, a_heading in enumerate(headings):\n",
    "        heading += f'{a_heading:<{column_width}}'\n",
    "        if i != len(headings) - 1: heading += ' | '\n",
    "    else: heading += '   '\n",
    "\n",
    "    print(\"\\t\" + heading, flush=True)\n",
    "    print(f'\\t{\"-\"*len(heading)}', flush=True)\n",
    "\n",
    "    for i in range(len(values)):\n",
    "        row = f'\\t{row_headings[i]:<{column_width}}'\n",
    "        for j in range(values.shape[-1]):\n",
    "            value = floatFormat(values[i, j])\n",
    "            row += f' | {value:>{column_width}}'\n",
    "        print(row, flush=True)\n",
    "        \n",
    "def floatFormat(number):\n",
    "    return f'{number:.6f}' \n",
    "\n",
    "def timeFormat(start_time, end_time):   \n",
    "\n",
    "    total_time = end_time-start_time\n",
    "\n",
    "    days = total_time // (24 * 60 * 60) \n",
    "    total_time = total_time % (24 * 60 * 60)\n",
    "\n",
    "    hours = total_time // (60 * 60)\n",
    "    total_time = total_time % (60 * 60)\n",
    "\n",
    "    minutes = total_time // 60\n",
    "    \n",
    "    seconds =total_time % (60)\n",
    "\n",
    "    return int(days), int(hours), int(minutes), int(seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a4fde01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelSetup(parser, test=False):\n",
    "\n",
    "    config_file = parser.pop(\"config_file\")\n",
    "    print(f'configs/{config_library[config_file]}')\n",
    "    with open(f'configs/{config_library[config_file]}') as file:\n",
    "        args = json.load(file)\n",
    "\n",
    "    if \"restore_epoch\" in parser:\n",
    "        restore_epoch = parser.pop(\"restore_epoch\")\n",
    "    if \"resume\" in parser:\n",
    "        resume = parser.pop(\"resume\")\n",
    "    else: \n",
    "        resume = False\n",
    "    if \"feat\" in parser:\n",
    "        feat = parser.pop(\"feat\")\n",
    "    else:\n",
    "        feat = None\n",
    "    if \"dataset_path\" in parser:\n",
    "        dataset_path = parser.pop(\"dataset_path\")\n",
    "    else: \n",
    "        dataset_path = None\n",
    "    if \"base_path\" in parser:\n",
    "        base_path = parser.pop(\"base_path\")\n",
    "    else: \n",
    "        base_path = None\n",
    "    image_base = parser.pop(\"image_base\")\n",
    "    device = parser.pop(\"device\")\n",
    "\n",
    "    for key in parser:\n",
    "        args[key] = parser[key]\n",
    "\n",
    "    args[\"data_train\"] = Path(args[\"data_train\"])\n",
    "    args[\"data_val\"] = Path(args[\"data_val\"])\n",
    "    args[\"data_test\"] = Path(args[\"data_test\"])\n",
    "\n",
    "    modelHash(args)\n",
    "\n",
    "    base_dir = Path(\"model_metadata\")    \n",
    "    data = \"_\".join(str(Path(os.path.basename(args[\"data_train\"])).stem).split(\"_\")[0:4])\n",
    "    model_particulars = f'AudioModel-{args[\"audio_model\"][\"name\"]}_ImageModel-{args[\"image_model\"]}_ArgumentsHash-{args[\"model_name\"]}_ConfigFile-{Path(config_library[config_file]).stem}' \n",
    "    args[\"exp_dir\"] = base_dir / data / model_particulars\n",
    "\n",
    "    if test or resume:\n",
    "\n",
    "        print(f'\\nRecovering model arguments from')\n",
    "        printDirectory(args[\"exp_dir\"] / \"args.pkl\")\n",
    "\n",
    "        print((args[\"exp_dir\"] / \"args.pkl\").absolute())\n",
    "        assert(os.path.isfile((args[\"exp_dir\"] / \"args.pkl\").absolute()))\n",
    "        with open(args[\"exp_dir\"] / \"args.pkl\", \"rb\") as f:\n",
    "            args = pickle.load(f)\n",
    "        \n",
    "        for key in parser:\n",
    "            args[key] = parser[key]\n",
    "\n",
    "        if restore_epoch != -1: args[\"restore_epoch\"] = restore_epoch\n",
    "        args[\"resume\"] = resume\n",
    "        if dataset_path is not None: args[\"dataset_path\"] = dataset_path\n",
    "        if base_path is not None: args[\"base_path\"] = base_path\n",
    "\n",
    "    else:\n",
    "        assert(os.path.isfile(args[\"exp_dir\"]) is False)\n",
    "        print(f'\\nMaking model directory:')\n",
    "        printDirectory(args[\"exp_dir\"])\n",
    "        print(f'Saving model arguments at:')\n",
    "        printDirectory(args[\"exp_dir\"] / \"args.pkl\")\n",
    "\n",
    "        os.makedirs(args[\"exp_dir\"])\n",
    "        with open(args[\"exp_dir\"] / \"args.pkl\", \"wb\") as f:\n",
    "            pickle.dump(args, f)\n",
    "        args[\"resume\"] = False\n",
    "    args[\"device\"] = device\n",
    "    if feat is not None:\n",
    "        args['feat'] = feat\n",
    "\n",
    "    print(f'Model arguments:')\n",
    "    printArguments(args)\n",
    "\n",
    "    getDevice(args)\n",
    "\n",
    "    return args, image_base\n",
    "\n",
    "\n",
    "def modelHash(args):\n",
    "\n",
    "    exclude_keys = [\"resume\"]\n",
    "\n",
    "    name_dict = args.copy()\n",
    "    name_dict.pop(\"resume\", None)\n",
    "\n",
    "    args[\"model_name\"] = hashlib.md5(repr(sorted(name_dict.items())).encode(\"ascii\")).hexdigest()[:10]\n",
    "image_model_dict = {\n",
    "    \"VGG16\": VGG16,\n",
    "    \"Resnet50\": Resnet50,\n",
    "    \"Resnet101\": Resnet101\n",
    "}\n",
    "audio_model_dict = {\n",
    "    \"Davenet\": AudioCNN,\n",
    "    \"ResDavenet\": ResDavenet,\n",
    "    \"Transformer\": BidrectionalAudioLSTM\n",
    "}\n",
    "\n",
    "def imageModel(args):\n",
    "    if args[\"image_model\"] == \"VGG16\":\n",
    "        return image_model_dict[\"VGG16\"]\n",
    "    elif args[\"image_model\"] == \"Resnet50\":\n",
    "        return image_model_dict[\"Resnet50\"]\n",
    "    elif args[\"image_model\"] == \"Resnet101\":\n",
    "        return image_model_dict[\"Resnet101\"]\n",
    "    else:\n",
    "        raise ValueError(f'Unknown image model: {args[\"image_model\"][\"name\"]}')\n",
    "\n",
    "def audioModel(args):\n",
    "    if args[\"audio_model\"][\"name\"] == \"DAVEnet\":\n",
    "        with open(f'models/DAVEnet.json') as file: model_params = json.load(file)\n",
    "        args[\"audio_model\"][\"conv_layers\"] = model_params[\"conv_layers\"]\n",
    "        args[\"audio_model\"][\"max_pool\"] = model_params[\"max_pool\"]\n",
    "        return audio_model_dict[\"Davenet\"]\n",
    "    elif args[\"audio_model\"][\"name\"] == \"ResDAVEnet\":\n",
    "        with open(f'models/ResDAVEnet.json') as file: model_params = json.load(file)\n",
    "        args[\"audio_model\"][\"conv_layers\"] = model_params[\"conv_layers\"]\n",
    "        return audio_model_dict[\"ResDavenet\"]\n",
    "    elif args[\"audio_model\"][\"name\"] == \"Transformer\":\n",
    "        return audio_model_dict[\"Transformer\"]\n",
    "    else:\n",
    "        raise ValueError(f'Unknown audio model: {args[\"audio_model\"][\"name\"]}')\n",
    "\n",
    "def acousticModel(args):\n",
    "    with open(f'models/AcousticEncoder.json') as file: model_params = json.load(file)\n",
    "    args[\"acoustic_model\"] = model_params\n",
    "\n",
    "def loadPretrainedWeights(acoustic_model, args):\n",
    "    \n",
    "    device = torch.device(args[\"device\"] if torch.cuda.is_available() else \"cpu\")\n",
    "    acoustic_model = nn.DataParallel(acoustic_model)\n",
    "    model_dict = acoustic_model.state_dict()\n",
    "    \n",
    "    cpc_pretrained_name = args['cpc']['pretrained_weights']\n",
    "    checkpoint_fn = Path(f'pretrained_cpc/{cpc_pretrained_name}.pt')\n",
    "    checkpoint = torch.load(checkpoint_fn, map_location=device)\n",
    "    \n",
    "    for key in checkpoint[\"acoustic_model\"]:\n",
    "        if key in model_dict: model_dict[key] = checkpoint[\"acoustic_model\"][key]\n",
    "    acoustic_model.load_state_dict(model_dict)\n",
    "    \n",
    "    return acoustic_model\n",
    "\n",
    "def getParameters(models, to_freeze, args):\n",
    "    valid_models = []\n",
    "    for model_name in models:\n",
    "        valid_models.append(\n",
    "            {\n",
    "            'params': models[model_name].parameters(),\n",
    "            'lr': args[\"learning_rate_scheduler\"][\"initial_learning_rate\"],\n",
    "            'name': model_name\n",
    "            }\n",
    "            )\n",
    "\n",
    "    for model_name in to_freeze:\n",
    "        for n, p in to_freeze[model_name].named_parameters(): \n",
    "            if n.startswith('embedder'):\n",
    "                valid_models.append(\n",
    "                {\n",
    "                'params': p,\n",
    "                'lr': args[\"learning_rate_scheduler\"][\"initial_learning_rate\"],\n",
    "                'name': model_name + \"_\" + n\n",
    "                }\n",
    "                )\n",
    "\n",
    "    return valid_models\n",
    "\n",
    "def loadModelAttriburesAndTrainingAMP(\n",
    "    exp_dir, acoustic_model, english_audio_model, hindi_audio_model, image_model, \n",
    "    optimizer, amp, device, last_not_best=True\n",
    "    ):\n",
    "\n",
    "    info_fn = exp_dir / \"training_metadata.json\"\n",
    "    with open(info_fn, \"r\") as f:\n",
    "        info = json.load(f)\n",
    "\n",
    "    if last_not_best:\n",
    "        checkpoint_fn = exp_dir / \"models\" / \"last_ckpt.pt\"\n",
    "    else:\n",
    "        checkpoint_fn = exp_dir / \"models\" / \"best_ckpt.pt\"\n",
    "\n",
    "    checkpoint = torch.load(checkpoint_fn, map_location=device)\n",
    "    \n",
    "    acoustic_model.load_state_dict(checkpoint[\"acoustic_model\"])\n",
    "    english_audio_model.load_state_dict(checkpoint[\"english_audio_model\"])\n",
    "    hindi_audio_model.load_state_dict(checkpoint[\"hindi_audio_model\"])\n",
    "    image_model.load_state_dict(checkpoint[\"image_model\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "    amp.load_state_dict(checkpoint[\"amp\"])\n",
    "    epoch = checkpoint[\"epoch\"]\n",
    "    global_step = checkpoint[\"global_step\"]\n",
    "    best_epoch = checkpoint[\"best_epoch\"]\n",
    "    best_acc = checkpoint[\"best_acc\"]  \n",
    "    print(f'\\nLoading model parameters from:\\n\\t\\t{checkpoint_fn}')\n",
    "\n",
    "    return info, epoch, global_step, best_epoch, best_acc\n",
    "\n",
    "\n",
    "def NFrames(audio_input, audio_output, nframes, with_torch=True):\n",
    "    pooling_ratio = round(audio_input.size(-1) / audio_output.size(-1))\n",
    "    if with_torch: pooling_ratio = torch.tensor(pooling_ratio, dtype=torch.int32)\n",
    "    nframes = nframes.float()\n",
    "    nframes.div_(pooling_ratio)\n",
    "    nframes = nframes.int()\n",
    "    zeros = (nframes == 0).nonzero()\n",
    "    if zeros.nelement() != 0: nframes[zeros[:, 0]] += 1\n",
    "\n",
    "    return nframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01fa4a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "command_line_args = {\n",
    "    \"resume\": True, \n",
    "    \"config_file\": 'multilingual+matchmap',\n",
    "    \"device\": \"0\", \n",
    "    \"restore_epoch\": -1, \n",
    "    \"image_base\": \"..\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd6b2103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "configs/English_Hindi_matchmap_DAVEnet_config.json\n",
      "\n",
      "Recovering model arguments from\n",
      "     model_metadata\n",
      "       ↪ PlacesAudio_400k_distro+PlacesHindi100k+imagesPlaces205_resize\n",
      "        ↪ AudioModel-Transformer_ImageModel-Resnet50_ArgumentsHash-57f998f2bf_ConfigFile-English_Hindi_matchmap_DAVEnet_config\n",
      "         ↪ args.pkl\n",
      "\n",
      "\n",
      "/home/leannenortje/SemanticAcousticModel/model_metadata/PlacesAudio_400k_distro+PlacesHindi100k+imagesPlaces205_resize/AudioModel-Transformer_ImageModel-Resnet50_ArgumentsHash-57f998f2bf_ConfigFile-English_Hindi_matchmap_DAVEnet_config/args.pkl\n",
      "Model arguments:\n",
      "\talphas: [1.5, 1.2, 1.5, 1.2, 2.5, 2.5]\n",
      "\taudio_config:\n",
      "\t\taudio_type: melspectrogram\n",
      "\t\tfmin: 20\n",
      "\t\tnum_mel_bins: 40\n",
      "\t\tpadval: 0\n",
      "\t\tpreemph_coef: 0.97\n",
      "\t\tsample_rate: 16000\n",
      "\t\ttarget_length: 1024\n",
      "\t\tuse_raw_length: False\n",
      "\t\twindow_size: 0.025\n",
      "\t\twindow_stride: 0.01\n",
      "\t\twindow_type: hamming\n",
      "\n",
      "\n",
      "\taudio_model:\n",
      "\t\tc_dim: 512\n",
      "\t\tembedding_dim: 2048\n",
      "\t\tname: Transformer\n",
      "\t\tnum_heads: 8\n",
      "\t\tz_dim: 64\n",
      "\n",
      "\n",
      "\tbatch_size: 32\n",
      "\tcpc:\n",
      "\t\thop_length: 160\n",
      "\t\tload_pretrained_weights: True\n",
      "\t\tn_negatives: 17\n",
      "\t\tn_prediction_steps: 6\n",
      "\t\tn_sample_frames: 128\n",
      "\t\tn_speakers_per_batch: 4\n",
      "\t\tn_utterances_per_speaker: 8\n",
      "\t\tpretrained_weights: epoch_1500\n",
      "\t\twarm_start: True\n",
      "\n",
      "\n",
      "\tdata_test: data/PlacesAudio_400k_distro+PlacesHindi100k+imagesPlaces205_resize_test.json\n",
      "\tdata_train: data/PlacesAudio_400k_distro+PlacesHindi100k+imagesPlaces205_resize_train.json\n",
      "\tdata_val: data/PlacesAudio_400k_distro+PlacesHindi100k+imagesPlaces205_resize_val.json\n",
      "\tdevice: 0\n",
      "\texp_dir: model_metadata/PlacesAudio_400k_distro+PlacesHindi100k+imagesPlaces205_resize/AudioModel-Transformer_ImageModel-Resnet50_ArgumentsHash-57f998f2bf_ConfigFile-English_Hindi_matchmap_DAVEnet_config\n",
      "\timage_config:\n",
      "\t\tRGB_mean: [0.485, 0.456, 0.406]\n",
      "\t\tRGB_std: [0.229, 0.224, 0.225]\n",
      "\t\tcenter_crop: False\n",
      "\t\tcrop_size: 224\n",
      "\n",
      "\n",
      "\timage_model: Resnet50\n",
      "\tlearning_rate_scheduler:\n",
      "\t\tdecay_every_n_epochs: 10\n",
      "\t\tdecay_factor: 0.95\n",
      "\t\tinitial_learning_rate: 0.0001\n",
      "\t\tlearning_rates: [0.0001, 0.0002, 0.0001, 5e-05]\n",
      "\t\tnum_epochs: [20, 50, 70, 80]\n",
      "\n",
      "\n",
      "\tloss: matchmap\n",
      "\tmargin: 1.0\n",
      "\tmodel_name: 57f998f2bf\n",
      "\tmomentum: 0.9\n",
      "\tn_epochs: 100\n",
      "\toptimizer: adam\n",
      "\tpretrained_image_model: True\n",
      "\tresume: True\n",
      "\tsimtype: MISA\n",
      "\tweight_decay: 5e-07\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "restore_epoch = command_line_args['restore_epoch']\n",
    "args, image_base = modelSetup(command_line_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1f8c4bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
      "\n",
      "Loading model parameters from:\n",
      "\t\tmodel_metadata/PlacesAudio_400k_distro+PlacesHindi100k+imagesPlaces205_resize/AudioModel-Transformer_ImageModel-Resnet50_ArgumentsHash-57f998f2bf_ConfigFile-English_Hindi_matchmap_DAVEnet_config/models/best_ckpt.pt\n",
      "epoch      = 1\n",
      "global_step = 3034\n",
      "best_epoch = 1\n",
      "best_acc   = 0.06879739978331527\n",
      "\n",
      "\n",
      "Recall scores at epoch 1:\n",
      "\tRestoring audio model from model_metadata/PlacesAudio_400k_distro+PlacesHindi100k+imagesPlaces205_resize/AudioModel-Transformer_ImageModel-Resnet50_ArgumentsHash-57f998f2bf_ConfigFile-English_Hindi_matchmap_DAVEnet_config/models/audio_model.1.pth\n",
      "\tRestoring audio model from model_metadata/PlacesAudio_400k_distro+PlacesHindi100k+imagesPlaces205_resize/AudioModel-Transformer_ImageModel-Resnet50_ArgumentsHash-57f998f2bf_ConfigFile-English_Hindi_matchmap_DAVEnet_config/models/image_model.1.pth\n"
     ]
    }
   ],
   "source": [
    "acousticModel(args)\n",
    "acoustic_model = AcousticEncoder(args).to(\"cpu\")\n",
    "# summary(acoustic_model, (40, 2048), device=\"cpu\")#, depth=5)\n",
    "\n",
    "audio_model_name = audioModel(args) \n",
    "english_audio_model = audio_model_name(args).to(\"cpu\")\n",
    "\n",
    "hindi_audio_model = audio_model_name(args).to(\"cpu\")\n",
    "\n",
    "image_model_name = imageModel(args)\n",
    "image_model = image_model_name(args, pretrained=args[\"pretrained_image_model\"]).to(\"cpu\")\n",
    "\n",
    "device = torch.device(args[\"device\"] if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "acoustic_model = acoustic_model.to(device)\n",
    "english_audio_model = english_audio_model.to(device)\n",
    "hindi_audio_model = hindi_audio_model.to(device)\n",
    "image_model = image_model.to(device)\n",
    "\n",
    "model_with_params_to_update = {\n",
    "    \"acoustic_model\": acoustic_model, \n",
    "    \"english_audio_model\": english_audio_model, \n",
    "    \"hindi_audio_model\": hindi_audio_model\n",
    "    }\n",
    "model_to_freeze = {\n",
    "    \"image_model\": image_model\n",
    "    }\n",
    "trainable_parameters = getParameters(model_with_params_to_update, model_to_freeze, args)\n",
    "\n",
    "if args[\"optimizer\"] == 'sgd':\n",
    "    optimizer = torch.optim.SGD(\n",
    "        trainable_parameters, args[\"learning_rate_scheduler\"][\"initial_learning_rate\"],\n",
    "        momentum=args[\"momentum\"], weight_decay=args[\"weight_decay\"]\n",
    "        )\n",
    "elif args[\"optimizer\"] == 'adam':\n",
    "    optimizer = torch.optim.Adam(\n",
    "        trainable_parameters, args[\"learning_rate_scheduler\"][\"initial_learning_rate\"],\n",
    "        weight_decay=args[\"weight_decay\"]\n",
    "        )\n",
    "else:\n",
    "    raise ValueError('Optimizer %s is not supported' % args[\"optimizer\"])\n",
    "\n",
    "[acoustic_model, english_audio_model, hindi_audio_model, image_model], optimizer = amp.initialize(\n",
    "        [acoustic_model, english_audio_model, hindi_audio_model, image_model], optimizer, opt_level='O1'\n",
    "        )\n",
    "\n",
    "acoustic_model = nn.DataParallel(acoustic_model) #if not isinstance(acoustic_model, torch.nn.DataParallel) and args[\"device\"] == 'cuda' else acoustic_model\n",
    "english_audio_model = nn.DataParallel(english_audio_model) if not isinstance(english_audio_model, torch.nn.DataParallel) and args[\"device\"] == 'cuda' else english_audio_model\n",
    "hindi_audio_model = nn.DataParallel(hindi_audio_model) if not isinstance(hindi_audio_model, torch.nn.DataParallel) and args[\"device\"] == 'cuda' else hindi_audio_model\n",
    "image_model = nn.DataParallel(image_model) if not isinstance(image_model, torch.nn.DataParallel) and args[\"device\"] == 'cuda' else image_model\n",
    "\n",
    "info, epoch, global_step, best_epoch, best_acc = loadModelAttriburesAndTrainingAMP(\n",
    "    args[\"exp_dir\"], acoustic_model, english_audio_model, hindi_audio_model, image_model, optimizer, amp, \n",
    "    device, False\n",
    ")\n",
    "print(f'{\"epoch\": <10} = {epoch}')\n",
    "print(f'{\"global_step\": <10} = {global_step}')\n",
    "print(f'{\"best_epoch\": <10} = {best_epoch}')\n",
    "print(f'{\"best_acc\": <10} = {best_acc}\\n')\n",
    "\n",
    "# assert (args[\"restore_epoch\"] <= args[\"n_epochs\"])\n",
    "\n",
    "print(f'\\nRecall scores at epoch {epoch}:')\n",
    "print(f'\\tRestoring audio model from {args[\"exp_dir\"]}/models/audio_model.{best_epoch}.pth')\n",
    "print(f'\\tRestoring audio model from {args[\"exp_dir\"]}/models/image_model.{best_epoch}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8d60a029",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_matchmap_similarity_matrix_IA(im, im_mask, audio, frames, simtype='MISA'):\n",
    "    \n",
    "    w = im.size(2)\n",
    "    h = im.size(3)\n",
    "    im = im.view(im.size(0), im.size(1), -1).transpose(1, 2)\n",
    "    audio = audio.squeeze(2)\n",
    "\n",
    "    assert(im.dim() == 3)\n",
    "    assert(audio.dim() == 3)\n",
    "    \n",
    "    n = im.size(0)\n",
    "\n",
    "    for i in range(n):\n",
    "        nth_entrance_in_audio = torch.cat(n*[audio[i, :, 0:frames[i]].unsqueeze(0)])\n",
    "\n",
    "        M = torch.bmm(im, nth_entrance_in_audio)\n",
    "        # M = torch.tanh(M)\n",
    "        M = M.view(im.size(0), w, h, -1)#.transpose(1, 2)\n",
    "\n",
    "        assert(M.dim() == 4)\n",
    "        if simtype == 'SISA':\n",
    "            M = M.mean((3))\n",
    "            M = M.mean((2))\n",
    "            M = M.mean((1))\n",
    "        elif simtype == 'MISA':\n",
    "            M, _ = M.max(1)\n",
    "            M, _ = M.max(1)\n",
    "            M = M.mean((1))\n",
    "        elif simtype == 'SIMA':\n",
    "            M, _ = M.max(3)\n",
    "            M =  M.mean((2))\n",
    "            M =  M.mean((1))\n",
    "        else:\n",
    "            raise ValueError\n",
    "\n",
    "        if i == 0: S = M.unsqueeze(1)\n",
    "        else: S = torch.cat([S, M.unsqueeze(1)], dim=1) \n",
    "    return S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0183c61e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Read in data paths from:\n",
      "     data\n",
      "       ↪ PlacesAudio_400k_distro+PlacesHindi100k+imagesPlaces205_resize_test.json\n",
      "\n",
      "\n",
      "\n",
      "\r",
      "Read in 914 data points\n"
     ]
    }
   ],
   "source": [
    "args[\"image_config\"][\"center_crop\"] = True\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    ImageAudioData(\n",
    "        image_base, args[\"data_test\"], args),\n",
    "    batch_size=args[\"batch_size\"], shuffle=False, num_workers=8, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2b3dac1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                       \r"
     ]
    }
   ],
   "source": [
    "device = torch.device(args[\"device\"] if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "image_model.eval()\n",
    "acoustic_model.eval()\n",
    "english_audio_model.eval()\n",
    "\n",
    "N_examples = test_loader.dataset.__len__()\n",
    "I_embeddings = [] \n",
    "E_embeddings = [] \n",
    "H_embeddings = [] \n",
    "E_frame_counts = []\n",
    "H_frame_counts = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (image_input, english_input, english_nframes, hindi_input, hindi_nframes) in tqdm(enumerate(test_loader), desc='\\tValidating...', leave=False):\n",
    "\n",
    "        image_output = image_model(image_input.to(device))#.to('cpu').detach()\n",
    "        I_embeddings.append(image_output)\n",
    "\n",
    "        english_z, english_c = acoustic_model(english_input.to(device))\n",
    "        english_output = english_audio_model(english_c.to(device))#.to('cpu').detach()\n",
    "        E_embeddings.append(english_output)\n",
    "        E_frame_counts.append(NFrames(english_input, english_output, english_nframes))#.cpu())\n",
    "\n",
    "        hindi_z, hindi_c = acoustic_model(hindi_input.to(device))\n",
    "        hindi_output = hindi_audio_model(hindi_c.to(device))#.to('cpu').detach()\n",
    "        H_embeddings.append(hindi_output)\n",
    "        H_frame_counts.append(NFrames(hindi_input, hindi_output, hindi_nframes))#.cpu())\n",
    "\n",
    "\n",
    "    heading = [\" \"]\n",
    "    r_at_10 = []\n",
    "    r_at_5 = []\n",
    "    r_at_1 = []\n",
    "    acc = 0\n",
    "    divide = 0\n",
    "\n",
    "    image_output = (torch.cat(I_embeddings))\n",
    "\n",
    "    english_output = (torch.cat(E_embeddings))\n",
    "    english_frames = (torch.cat(E_frame_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f4021c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "S = compute_matchmap_similarity_matrix_IA(image_output, None, english_output, english_frames, simtype=args['simtype'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "61e47787",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t           | E -> I     | I -> E       \n",
      "\t---------------------------------------\n",
      "\tR@10       |   0.105033 |   0.115974\n",
      "\tR@5        |   0.056893 |   0.066740\n",
      "\tR@1        |   0.014223 |   0.015317\n",
      "Retrieval accuracy: 11.050328227571116%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# def calc_recalls_IA(A, B, B_mask, attention, simtype='MISA'):\n",
    "#     # function adapted from https://github.com/dharwath\n",
    "\n",
    "#     S = compute_matchmap_similarity_matrix_IA(A, B, B_mask, attention, simtype=simtype)\n",
    "#     n = S.size(0)\n",
    "#     I2A_scores, I2A_ind = S.topk(10, 1)\n",
    "#     A2I_scores, A2I_ind = S.topk(10, 0)\n",
    "\n",
    "#     I2A_scores = I2A_scores.detach().cpu().numpy()\n",
    "#     I2A_ind = I2A_ind.detach().cpu().numpy()\n",
    "#     A2I_scores = A2I_scores.detach().cpu().numpy()\n",
    "#     A2I_ind = A2I_ind.detach().cpu().numpy()\n",
    "\n",
    "#     A_foundind = -np.ones(n)\n",
    "#     I_foundind = -np.ones(n)\n",
    "#     for i in tqdm(range(n), desc=\"Calculating recalls\", leave=False):\n",
    "#         ind = np.where(I2A_ind[i, :] == i)[0]\n",
    "#         if len(ind) != 0: A_foundind[i] = ind[0]\n",
    "#         ind = np.where(A2I_ind[:, i] == i)[0]\n",
    "#         if len(ind) != 0: I_foundind[i] = ind[0]\n",
    " \n",
    "#     r1_I2A = len(np.where(A_foundind == 0)[0])/len(A_foundind)\n",
    "#     r5_I2A = len(np.where(np.logical_and(A_foundind >= 0, A_foundind < 5))[0])/len(A_foundind)\n",
    "#     r10_I2A = len(np.where(np.logical_and(A_foundind >= 0, A_foundind < 10))[0])/len(A_foundind)\n",
    "\n",
    "#     r1_A2I = len(np.where(I_foundind == 0)[0])/len(I_foundind)\n",
    "#     r5_A2I = len(np.where(np.logical_and(I_foundind >= 0, I_foundind < 5))[0])/len(I_foundind)\n",
    "#     r10_A2I = len(np.where(np.logical_and(I_foundind >= 0, I_foundind < 10))[0])/len(I_foundind)\n",
    "\n",
    "#     return {\n",
    "#         'r1_I2A':r1_I2A, \n",
    "#         'r5_I2A':r5_I2A, \n",
    "#         'r10_I2A':r10_I2A,\n",
    "#         'r1_A2I':r1_A2I, \n",
    "#         'r5_A2I':r5_A2I, \n",
    "#         'r10_A2I':r10_A2I\n",
    "#         }\n",
    "\n",
    "n = S.size(0)\n",
    "S = S.to(device)\n",
    "A2B_scores, A2B_ind = S.topk(10, 1)\n",
    "B2A_scores, B2A_ind = S.topk(10, 0)\n",
    "\n",
    "A2B_scores = A2B_scores.detach().cpu().numpy()\n",
    "A2B_ind = A2B_ind.detach().cpu().numpy()\n",
    "B2A_scores = B2A_scores.detach().cpu().numpy()\n",
    "B2A_ind = B2A_ind.detach().cpu().numpy()\n",
    "\n",
    "A_foundind = -np.ones(n)\n",
    "B_foundind = -np.ones(n)\n",
    "for i in tqdm(range(n), desc=\"Calculating recalls\", leave=False):\n",
    "    ind = np.where(A2B_ind[i, :] == i)[0]\n",
    "    if len(ind) != 0: B_foundind[i] = ind[0]\n",
    "    ind = np.where(B2A_ind[:, i] == i)[0]\n",
    "    if len(ind) != 0: A_foundind[i] = ind[0]\n",
    "\n",
    "r1_A_to_B = len(np.where(B_foundind == 0)[0])/len(B_foundind)\n",
    "r5_A_to_B = len(np.where(np.logical_and(B_foundind >= 0, B_foundind < 5))[0])/len(B_foundind)\n",
    "r10_A_to_B = len(np.where(np.logical_and(B_foundind >= 0, B_foundind < 10))[0])/len(B_foundind)\n",
    "\n",
    "r1_B_to_A = len(np.where(A_foundind == 0)[0])/len(A_foundind)\n",
    "r5_B_to_A = len(np.where(np.logical_and(A_foundind >= 0, A_foundind < 5))[0])/len(A_foundind)\n",
    "r10_B_to_A = len(np.where(np.logical_and(A_foundind >= 0, A_foundind < 10))[0])/len(A_foundind)\n",
    "\n",
    "\n",
    "recalls = {\n",
    "        'A_to_B_r1':r1_A_to_B, \n",
    "        'A_to_B_r5':r5_A_to_B, \n",
    "        'A_to_B_r10':r10_A_to_B,\n",
    "        'B_to_A_r1':r1_B_to_A, \n",
    "        'B_to_A_r5':r5_B_to_A, \n",
    "        'B_to_A_r10':r10_B_to_A\n",
    "        }\n",
    "\n",
    "heading = [\" \"]\n",
    "r_at_10 = []\n",
    "r_at_5 = []\n",
    "r_at_1 = []\n",
    "acc = 0\n",
    "divide = 0\n",
    "\n",
    "heading.extend([\"E -> I\", \"I -> E\"])\n",
    "r_at_10.extend([recalls[\"B_to_A_r10\"], recalls[\"A_to_B_r10\"]])\n",
    "r_at_5.extend([recalls[\"B_to_A_r5\"], recalls[\"A_to_B_r5\"]])\n",
    "r_at_1.extend([recalls[\"B_to_A_r1\"], recalls[\"A_to_B_r1\"]])\n",
    "acc += recalls[\"B_to_A_r10\"] + recalls[\"A_to_B_r10\"]\n",
    "divide += 2\n",
    "\n",
    "tablePrinting(\n",
    "    heading, [\"R@10\", \"R@5\", \"R@1\"],\n",
    "    np.asarray([r_at_10, r_at_5, r_at_1])\n",
    "    )\n",
    "\n",
    "print(f'Retrieval accuracy: {100 * acc / divide}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d361599e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(I2A_scores[10, :])\n",
    "print(I2A_ind[10, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0642f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "S[10, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570ca5ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyterEnv",
   "language": "python",
   "name": "jupyterenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
